{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gustavocac/COVID_handson/blob/master/COVID_AI_Dasa_Educa.ipynb\" target=\"_parent\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uH4B_VCqjKy3"
   },
   "source": [
    "![image](https://github.com/gustavocac/COVID_hanson/blob/master/cdc-k0KRNtqcjfw-unsplash.jpg?raw=true)\n",
    "Photo by CDC on Unsplash\n",
    "\n",
    "\n",
    "# COVID 2019 \n",
    "\n",
    "# Hands-on - Deep Learning for COVID Detection on CT \n",
    "\n",
    "\n",
    "\n",
    "**Developed by:**\n",
    "Gustavo C. A. Corradi\n",
    "\n",
    "Originally Branched and adapted from:\n",
    "\n",
    "https://colab.research.google.com/github/gustavocac/rsna_handson/blob/master/RSNA_for_Non_Coders_final_version.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbAUq1KmksS9"
   },
   "source": [
    "All the process will be demonstrated with Python 3 running on Google Colaboratory. \n",
    "Please make sure you have GPU enabled under notebook settings before you proceed.\n",
    "\n",
    "**There are 3 training sets:**\n",
    "\n",
    "- Dataset 0 comprises 60 normal and 6 COVID CT images.\n",
    "\n",
    "- Dataset 1 comprises 33 normal and 33 COVID CT images.\n",
    "\n",
    "- Dataset 2 comprises 60 normal and 60 COVID CT images.\n",
    "\n",
    "Validation and Test sets have 20 normal and 20 COVID, except for dataset 0 (20 normal and 2 COVID).\n",
    "\n",
    "For each specific task we will import specific libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation the the system will restart. This will generate an error message which is expected. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwFwbMGIuHsH"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_hkRJ8W-6xF"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "!pip3 install keras-vis\n",
    "!pip3 install imgaug==0.2.5\n",
    "!pip3 install scipy==1.2.1\n",
    "!pip3 install graphviz\n",
    "!pip uninstall matplotlib --yes\n",
    "!pip install matplotlib==3.1.0\n",
    "!pip install academictorrents\n",
    "!pip install html2text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9dmrWVwRbDR"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/323a0048d87ca79b68f12a6350a57776b6a3b7fb.torrent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\academictorrents\\Torrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hash, datastore)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tmp/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhash\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.torrent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/323a0048d87ca79b68f12a6350a57776b6a3b7fb.torrent'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-55470227e4b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ls'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#https://github.com/ieee8023/covid-chestxray-dataset/blame/master/volumes/DOWNLOAD.md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpath_of_giant_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"323a0048d87ca79b68f12a6350a57776b6a3b7fb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Download mnist dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#https://drive.google.com/file/d/1SJoMelgRqb0EuqlTuq6dxBWf2j9Kno8S/view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wget https://github.com/ieee8023/covid-chestxray-dataset   '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\academictorrents\\academictorrents.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(at_hash, datastore, urls, showlogs, use_timestamp)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdatastore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"~/.academictorrents-datastore\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtorrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTorrent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mat_hash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatastore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtorrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murls\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatastore\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'info'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\academictorrents\\Torrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hash, datastore)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not find a torrent with this hash on the tracker or in the data directory:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\academictorrents\\Torrent.py\u001b[0m in \u001b[0;36mget_from_url\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mtorrent_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/tmp/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.torrent'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorrent_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorrent_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/323a0048d87ca79b68f12a6350a57776b6a3b7fb.torrent'"
     ]
    }
   ],
   "source": [
    "#@title Dataset Download from Github https://github.com/ieee8023/covid-chestxray-dataset/blob/master/ \n",
    "\n",
    "\n",
    "import sys, os\n",
    "import academictorrents as at\n",
    "default_stdout = sys.stdout\n",
    "file_handle = open(os.devnull, \"w\")\n",
    "sys.stderr = open(os.devnull, \"w\")\n",
    "\n",
    "sys.stdout = file_handle\n",
    "\n",
    "#@title\n",
    "#First of all, we are going to download the zip files with the images to this instance of Google's Colaboratory\n",
    "!cd \n",
    "!cd .Desktop/teste/positive\n",
    "!ls\n",
    "#https://github.com/ieee8023/covid-chestxray-dataset/blame/master/volumes/DOWNLOAD.md\n",
    "path_of_giant_dataset = at.get(\"323a0048d87ca79b68f12a6350a57776b6a3b7fb\") # Download mnist dataset\n",
    "#https://drive.google.com/file/d/1SJoMelgRqb0EuqlTuq6dxBWf2j9Kno8S/view\n",
    "!wget https://github.com/ieee8023/covid-chestxray-dataset   \n",
    "!wget https://github.com/kitamura-felipe/deeplearning_head_ct_demo/blob/master/33_33.zip?raw=true\n",
    "!wget https://github.com/kitamura-felipe/deeplearning_head_ct_demo/blob/master/60_6.zip?raw=true\n",
    "  \n",
    "  #@title\n",
    "#Let's unzip them\n",
    "\n",
    "\n",
    "\n",
    "!unzip -o Allcases.zip?raw=true -d /Cases\n",
    "!unzip -o 33_33.zip?raw=true -d /Cases\n",
    "!unzip -o 60_6.zip?raw=true -d /Cases\n",
    "\n",
    "\n",
    "#print ('\\033[1m' + 'All images downloaded. Please check folder contents under \"Files\" on the left!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DOWNLOAD.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6d489ee9575c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhtml2text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wget https://github.com/ieee8023/covid-chestxray-dataset/blame/master/volumes/DOWNLOAD.md'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DOWNLOAD.md'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml2text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTML2Text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DOWNLOAD.md'"
     ]
    }
   ],
   "source": [
    "import html2text\n",
    "!wget https://github.com/ieee8023/covid-chestxray-dataset/blame/master/volumes/DOWNLOAD.md\n",
    "with open('DOWNLOAD.md', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "h = html2text.HTML2Text(url)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'urlopen' from 'urllib' (C:\\ProgramData\\Anaconda3\\lib\\urllib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8b7c22af90c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'urlopen' from 'urllib' (C:\\ProgramData\\Anaconda3\\lib\\urllib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import academictorrents as at\n",
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "print(\"Libraries imported, about to start a download\")\n",
    "print(soup)\n",
    "#filename = at.get(\"323a0048d87ca79b68f12a6350a57776b6a3b7fb\")\n",
    "\n",
    "print(\"About to open the file\")\n",
    "\n",
    "mnist = gzip.open(filename, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(mnist)\n",
    "mnist.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "LhGUx-McKJ1M"
   },
   "outputs": [],
   "source": [
    "#@title Reset for next experiment\n",
    "import os\n",
    "os.kill(os.getpid(), 9)\n",
    "import sys, os\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrsQG6zJHI5F"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Selection and Preprocessing {display-mode: \"form\"}\n",
    "import os\n",
    "\n",
    "Dataset = \"60/6\" #@param [\"60/6\", \"33/33\", \"60/60\"]\n",
    "Augmentation = \"off\" #@param [\"off\", \"on\", \"custom\"]\n",
    "angle = 359 #@param {type:\"slider\", min:0, max:359, step:1}\n",
    "zoom = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "width_shift_range = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "height_shift_range = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "shear_range = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "horizontal_flip = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
    "%tensorflow_version 1.x\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
    "# SET ALL THE SEEDS\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(123)\n",
    "os.environ['PYTHONHASHSEED'] = '123'\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# We split the data between 60/10/30% for training/validation/test sets \n",
    "# We choose which directories must be used\n",
    "\n",
    "validation_data_dir = '/Cases/All cases/Validation/'\n",
    "nb_validation_samples = 40\n",
    "\n",
    "test_data_dir = '/Cases/All cases/Test/'\n",
    "nb_test_samples = 40\n",
    "\n",
    "if Dataset == '60/6':\n",
    "  train_data_dir = '/Cases/60+6/Training/'\n",
    "  nb_train_samples = 66\n",
    "  test_data_dir = '/Cases/60+6/Test/'\n",
    "  nb_test_samples = 40\n",
    "  validation_data_dir = '/Cases/60+6/Validation/'\n",
    "  nb_validation_samples = 40\n",
    "elif Dataset == '33/33':\n",
    "  train_data_dir = '/Cases/33+33/Training/'\n",
    "  nb_train_samples = 66\n",
    "else:\n",
    "  train_data_dir = '/Cases/All cases/Training/'\n",
    "  nb_train_samples = 120\n",
    "\n",
    "ds_size = {\n",
    "    \"Train_Hematoma\": len(glob(train_data_dir + \"Hematoma/*\")),\n",
    "    \"Train_Normal\": len(glob(train_data_dir + \"Normal/*\")),\n",
    "    \"Val_Hematoma\": len(glob(validation_data_dir + \"Hematoma/*\")),\n",
    "    \"Val_Normal\": len(glob(validation_data_dir + \"Normal/*\")),\n",
    "    \"Test_Hematoma\": len(glob(test_data_dir + \"Hematoma/*\")),\n",
    "    \"Test_Normal\": len(glob(test_data_dir + \"Normal/*\")),\n",
    "}\n",
    "\n",
    "# For generator we need to give these two hyperparameters\n",
    "epochs = 40\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "# This is the augmentation configuration we will use for training\n",
    "dataaug = Augmentation\n",
    "\n",
    "if dataaug == \"off\":\n",
    "  print(\"Data Augmentation OFF\")\n",
    "  train_datagen = ImageDataGenerator(\n",
    "      rescale=1. / 255) # normalization\n",
    "elif dataaug == \"on\":\n",
    "  print(\"Data Augmentation ON\")\n",
    "  train_datagen = ImageDataGenerator(\n",
    "      rescale=1. / 255, # normalization\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=0.1,\n",
    "      rotation_range=20,\n",
    "      fill_mode=\"constant\",\n",
    "      horizontal_flip=True)  \n",
    "else:\n",
    "  print(\"Data Augmentation Custom\")\n",
    "  train_datagen = ImageDataGenerator(\n",
    "      rescale=1. / 255, # normalization\n",
    "      width_shift_range=width_shift_range,\n",
    "      height_shift_range=height_shift_range,\n",
    "      shear_range=shear_range,\n",
    "      fill_mode=\"constant\",\n",
    "      zoom_range=zoom, \n",
    "      rotation_range=angle,\n",
    "      horizontal_flip=horizontal_flip)\n",
    "\n",
    "# This is the augmentation configuration we will use for validation:\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
    "\n",
    "print(\"Training set:\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "print(\"Validation set:\")\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "print(\"Test set:\")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_test_samples,\n",
    "    class_mode='binary', shuffle = False)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#Let's plot the class frequencies\n",
    "\n",
    "print ('\\033[1m' + '\\r\\n          Training Set Distribution')\n",
    "plt.figure()\n",
    "plt.bar([0, 1], [ds_size['Train_Hematoma'], ds_size['Train_Normal']], color=['red', 'blue'])\n",
    "plt.xticks([0, 1], ('Hematoma', 'Normal'))\n",
    "plt.show()\n",
    "\n",
    "print ('\\033[1m' + '\\r\\n          Validation Set Distribution')\n",
    "plt.figure()\n",
    "plt.bar([0, 1], [ds_size['Val_Hematoma'], ds_size['Val_Normal']], color=['red', 'blue'])\n",
    "plt.xticks([0, 1], ('Hematoma', 'Normal'))\n",
    "plt.show()\n",
    "\n",
    "print ('\\033[1m' + '\\r\\n          Test Set Distribution')\n",
    "plt.figure()\n",
    "plt.bar([0, 1], [ds_size['Test_Hematoma'], ds_size['Test_Normal']], color=['red', 'blue'])\n",
    "plt.xticks([0, 1], ('Hematoma', 'Normal'))\n",
    "plt.show()\n",
    "\n",
    "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
    "\n",
    "print ('\\033[1m' + '\\r\\nNow let\\'s see some examples\\r\\n')\n",
    "\n",
    "x,y = train_generator.next()\n",
    "\n",
    "labley = y==0\n",
    "#shape = x.shape\n",
    "#print (shape)\n",
    "#for i in range(0, 8):\n",
    "#  plt.subplot(240 + 1 + i).grid(False)\n",
    "#  plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
    "#  plt.title(\"\\nLable:{}\".format(labley[i]))\n",
    "#  plt.axis('off')\n",
    "\n",
    "start_idx = 0\n",
    "fig, ax = plt.subplots(2,5, figsize=(15,8))\n",
    "for j in range(0,2): \n",
    "  for i in range(0,5):\n",
    "     ax[j][i].xaxis.set_major_locator(plt.NullLocator())\n",
    "     ax[j][i].yaxis.set_major_locator(plt.NullLocator())\n",
    "     ax[j][i].imshow(x[start_idx], cmap='gray')\n",
    "     ax[j][i].set_title(\"Index: {} \\nLabel: {}\".format(start_idx, 'Hematoma' if y[start_idx]==0 else 'Normal'))\n",
    "     start_idx +=1\n",
    "plt.show()\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "print ('\\033[1m' + 'Ready for next step!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZr5PnzuynjJ"
   },
   "source": [
    "<center> <img src=\"https://github.com/igorafaelms/rsna_handson/blob/master/transfer_learning.png?raw=true\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi3uEzBZHr0w"
   },
   "outputs": [],
   "source": [
    "#@title Setting Neural Net {display-mode: \"form\"}\n",
    "\n",
    "transfer_learning = \"VGG16+ImageNet\" #@param [\"None\", \"VGG16+ImageNet\"]\n",
    "\n",
    "# This code will be hidden when the notebook is loaded.\n",
    "# We can improve our results using transfer learning\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "if transfer_learning==\"None\":\n",
    "  base_model = VGG16(weights=None, include_top=False, input_shape=(img_width, img_height, 3))\n",
    "else:\n",
    "  base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "# Let's edit the last layers of VGG16 to use it in our solution\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "clear_output()\n",
    "from keras.models import Model\n",
    "clear_output()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "clear_output()\n",
    "from keras import optimizers\n",
    "clear_output()\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Only for version 2\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# And a logistic layer\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "SIIM_Net= Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# We can try using a different optimizer as well\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "SIIM_Net.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "clear_output()\n",
    "print ('\\033[1m' + 'Ready for next step!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpWTo4nSIPDy"
   },
   "outputs": [],
   "source": [
    "#@title Training and Validation {display-mode: \"form\"}\n",
    "epochs = 20 #@param {type:\"slider\", min:5, max:80, step:1}\n",
    "# This code will be hidden when the notebook is loaded.\n",
    "\n",
    "# Time to train it\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='Best_model.hdf5', monitor='val_loss',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "hist = SIIM_Net.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples // batch_size,\n",
    "            callbacks=[checkpointer])\n",
    "\n",
    "#Plotting the loss function\n",
    "\n",
    "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
    "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
    "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\033[1m' + \"Best Validation Accuracy: {:.2f}%\".format(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]*100.))\n",
    "print(\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5jVTnNKJfyj"
   },
   "outputs": [],
   "source": [
    "#@title Algorithm Performance (test set) - Score Histogram per Class { display-mode: \"form\" }\n",
    "\n",
    "# This code will be hidden when the notebook is loaded.\n",
    "\n",
    "from keras.models import load_model\n",
    "from tabulate import tabulate\n",
    "\n",
    "#Loading the best model\n",
    "\n",
    "best_model = load_model('Best_model.hdf5')\n",
    "\n",
    "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
    "\n",
    "labels_pred = best_model.predict(X) #predict the output from X\n",
    "\n",
    "labels_test = Y\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "Y_neg = (1-labels_pred[labels_test == 1])\n",
    "\n",
    "Y_pos = (1-labels_pred[labels_test == 0])\n",
    "headers = [\"Normal\", \"Hematoma\"]\n",
    "print(tabulate([[Y_neg,Y_pos]],headers,tablefmt=\"orgtbl\"))\n",
    "\n",
    "bins = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Y_neg, bins, alpha=0.4, label='Normal', color='b')\n",
    "plt.hist(Y_pos, bins, alpha=0.4, label='Hematoma', color='r')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()\n",
    "\n",
    "print ('\\033[1m' + 'Ready for next step!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJNi_Gts8dty"
   },
   "outputs": [],
   "source": [
    "#@title Algorithm Performance (test set) - 2 x 2 table { display-mode: \"form\" }\n",
    "threshold = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "threshold = 1-threshold\n",
    "# This code will be hidden when the notebook is loaded.\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "#Loading the best model\n",
    "\n",
    "best_model = load_model('Best_model.hdf5')\n",
    "\n",
    "\n",
    "# Defining a function to plot a confusion matrix.\n",
    "\n",
    "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = '2x2 table'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Ground Truth',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[1]):\n",
    "        for j in range(cm.shape[0]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
    "\n",
    "labels_pred = best_model.predict(X) #predict the output from X\n",
    "\n",
    "#labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
    "\n",
    "labels_pred = labels_pred >= threshold\n",
    "\n",
    "labels_test = Y\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Hematoma','Normal'], normalize=False,\n",
    "                      title='Confusion Matrix (Counts)')\n",
    "\n",
    "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100.))\n",
    "\n",
    "#print ('\\033[1m' + 'Ready for next step!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-z04WjMXWLE"
   },
   "outputs": [],
   "source": [
    "#@title Algorithm Performance (test set) - ROC Curve {display-mode: \"form\"}\n",
    "\n",
    "threshold = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "threshold = 1-threshold\n",
    "\n",
    "#Plotting the ROC curve with the AUC\n",
    "\n",
    "labels_pred = best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "\n",
    "plt.plot(fpr[np.argmin(np.abs(thresholds-threshold))],tpr[np.argmin(np.abs(thresholds-threshold))], 'o', color='red')\n",
    "\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
    "\n",
    "labels_pred = labels_pred >= threshold\n",
    "\n",
    "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=0, average='binary', sample_weight=None)\n",
    "\n",
    "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100.))\n",
    "\n",
    "print(\"F1 Score: {:.4f}\".format(f1_score))\n",
    "\n",
    "print ('\\033[1m' + 'Ready for next step!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7v2NUvMT7aib"
   },
   "outputs": [],
   "source": [
    "#@title Test Evaluation - Prediction on sampled images {display-mode: \"form\"}\n",
    "threshold = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "threshold = 1-threshold\n",
    "\n",
    "# Finally, we can use the test set for predictions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "test_data_dir = '/Cases/All cases/Test/' # location of test dataset\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "         rescale=1./255)       # normalize pixel values to [0,1]\n",
    "\n",
    "# Preparing test set images for prediction\n",
    "\n",
    "itr = test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=40,\n",
    "    shuffle='False',\n",
    "    class_mode='binary')\n",
    "batch_x, batch_y = itr.next()\n",
    "\n",
    "print('Test Accuracy: {:.2f}%'.format(100.*best_model.evaluate(batch_x, batch_y, verbose=0)[1]))\n",
    "\n",
    "\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "#prediction1 = np.round(best_model.predict(batch_x, verbose=1))==0\n",
    "prediction = best_model.predict(batch_x, verbose=1)\n",
    "\n",
    "start_idx = randrange(batch_x.shape[0]-10) \n",
    "fig, ax = plt.subplots(2,5, figsize=(15,8))\n",
    "for j in range(0,2): \n",
    "  for i in range(0,5):\n",
    "     ax[j][i].xaxis.set_major_locator(plt.NullLocator())\n",
    "     ax[j][i].yaxis.set_major_locator(plt.NullLocator())\n",
    "     ax[j][i].imshow(batch_x[start_idx], cmap='gray')\n",
    "     ax[j][i].set_title(\"Index: {} \\nPrediction: {} \\nValue: {:.2f}\".format(start_idx, 'Hematoma' if prediction[start_idx]<threshold else 'Normal', 1-prediction[start_idx][0]))\n",
    "     start_idx +=1\n",
    "plt.show()\n",
    "\n",
    "print ('\\033[1m' + 'Ready for next step!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JU3kUp6ccpVV"
   },
   "outputs": [],
   "source": [
    "#@title Visualization - Saliency Maps {display-mode: \"form\"}\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing visualization tools\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.layers import Input\n",
    "from keras import activations\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model\n",
    "from vis.visualization import visualize_activation,visualize_saliency,overlay,visualize_cam\n",
    "from vis.utils import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import imagenet_utils\n",
    "import numpy as np\n",
    "\n",
    "layer_idx = utils.find_layer_idx(best_model, 'block5_conv3')\n",
    "print(\"Remove Activation from Last Layer\")\n",
    "# Swap softmax with linear\n",
    "best_model.layers[layer_idx].activation = activations.linear\n",
    "print(\"Done. Now Applying changes to the model ...\")\n",
    "activation2_model = utils.apply_modifications(best_model)\n",
    "\n",
    "#print(activation_model.summary())\n",
    "#im_files=[\"/All cases/Test/Hematoma/Test hematoma (1).png\",\"/All cases/Test/Normal/Test_normal (1).png\"]\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "import os\n",
    "\n",
    "dir_name='/Cases/All cases/Test/'\n",
    "im_files = test_generator.filenames\n",
    "for im_file in im_files[3:6]:\n",
    "    img1 = image.load_img(dir_name + im_file,target_size=(150,150))\n",
    "    img1 = image.img_to_array(img1)\n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img1 = preprocess_input(img1)\n",
    "    layer_idx = utils.find_layer_idx(activation2_model, 'block5_conv3')\n",
    "    heatmap = visualize_cam(activation2_model, layer_idx, filter_indices=range(activation2_model.layers[layer_idx].filters), seed_input=img1[0,:,:,:])\n",
    "    img_init=utils.load_img(dir_name + im_file,target_size=(150,150))\n",
    "    img_init = img_init[:,:,:3]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax1 = plt.subplot(1,3,1)\n",
    "    ax1.grid(False)\n",
    "    plt.imshow(img_init, cmap='gray')\n",
    "    ax2 = plt.subplot(1,3,2)\n",
    "    ax2.grid(False)\n",
    "    plt.imshow(heatmap)\n",
    "    ax3 = plt.subplot(1,3,3)\n",
    "    ax3.grid(False)\n",
    "    plt.imshow(overlay(img_init, heatmap))\n",
    "    plt.show()\n",
    "    \n",
    "#print ('\\n' + '\\033[1m' + 'Congratulations, you have completed the assignment!')\n",
    "\n",
    "#from IPython.display import HTML\n",
    "#HTML('<img src=\"https://media.giphy.com/media/cub3pntkz8muQ/giphy.gif\">')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "COVID_AI for Non-Coders w comments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
